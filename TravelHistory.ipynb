{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "\n",
    "with urllib.request.urlopen(\n",
    "    \"https://api.steinhq.com/v1/storages/5e736c1db88d3d04ae0815b3/Raw_Data\"\n",
    ") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30              Travelled to Thailand and Malaysia\n",
       "31                             Travelled from Iran\n",
       "32                             Travelled from Iran\n",
       "33                             Travelled from Oman\n",
       "34    Travel from Italy on 29/02/2020 through Doha\n",
       "Name: Notes, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Notes\"][30:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy import displacy\n",
    "# from spacy.matcher import Matcher\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matched_sents = []  # Collect data of matched sentences to be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = [{\"LEMMA\": \"travel\"}, {\"LOWER\": \"from\"}, {\"POS\": \"PROPN\"}]\n",
    "# matcher.add(\"TravelledFrom\", None, pattern)\n",
    "# doc = nlp(\"Travelled from Italy and China\")\n",
    "# matches = matcher(doc)\n",
    "# matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for match_id, start, end in matches:\n",
    "#     string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "#     span = doc[start:end]  # The matched span\n",
    "#     print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.tokens import Token\n",
    "# def find_travelled_from(sents):\n",
    "#     matcher = Matcher(nlp.vocab)\n",
    "#     pattern = [{\"LEMMA\": \"travel\"}, {\"LOWER\": \"from\"}]\n",
    "#     matcher.add(\"TravelledFrom\", None, pattern)\n",
    "#     Token.set_extension(\"is_travelled_from\", default=False, force=True)\n",
    "#     for sent in sents:\n",
    "#         doc = nlp(sent)\n",
    "#         matches = matcher(doc)\n",
    "#         travelled_from = []\n",
    "#         for match_id, start, end in matches:\n",
    "#             if doc.vocab.strings[match_id] == \"TravelledFrom\":\n",
    "#                 travelled_from.append(doc[start:end])\n",
    "#         with doc.retokenize() as retokenizer:\n",
    "#             for span in travelled_from:\n",
    "#                 retokenizer.merge(span)\n",
    "#                 for token in span:\n",
    "#                     token._.is_travelled_from = True\n",
    "#         for token in doc:\n",
    "#             if token._.is_travelled_from:\n",
    "#                 print(token.text, token._.is_travelled_from)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sents = list(df[\"Notes\"][30:35])\n",
    "# find_travelled_from(input_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Notes\"][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_travelled_from(sents):\n",
    "#     matcher = Matcher(nlp.vocab)\n",
    "#     pattern = [{\"LEMMA\": \"travel\"}, {}]\n",
    "#     matcher.add(\"Travel\", None, pattern)\n",
    "#     for sent in sents:\n",
    "#         doc = nlp(sent)\n",
    "#         matches = matcher(doc)\n",
    "#         for match_id, start, end in matches:\n",
    "#             string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "#             span = doc[start:end]  # The matched span\n",
    "#             print(\"Input: \", sent, \"\\nPlace: \", span.text.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "def get_travel_status(span):\n",
    "    if span.label_ ==\"GPE\":\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"from\", \"through\", \"via\", \"Via\"):\n",
    "            return(\"from\")\n",
    "        elif prev_token.text in (\"to\", \"and\"):\n",
    "            return(\"to\")\n",
    "        return \"to\"\n",
    "\n",
    "# Register the Span extension as 'travel_status'\n",
    "Span.set_extension(\"travel_status\", getter=get_travel_status, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Indian', 'Indian')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "def get_nationality(span):\n",
    "    if span.label_ ==\"NORP\":\n",
    "        return span.text\n",
    "\n",
    "# Register the Span extension as 'nationality'\n",
    "Span.set_extension(\"nationality\", getter=get_nationality, force=True)\n",
    "\n",
    "doc = nlp(\"Indian Tourist\")\n",
    "print([(ent.text, ent._.nationality) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_travelled(sents):\n",
    "    for sent in sents:\n",
    "        s = re.sub(r'[^\\w\\s]',' ',sent)\n",
    "        doc = nlp(s)\n",
    "        print(sent)\n",
    "        for ent in doc.ents:\n",
    "            if ent._.travel_status:\n",
    "                print(f\"\\tTravel {ent._.travel_status} {ent.text}\")\n",
    "            if ent._.nationality:\n",
    "                print(f\"\\tNationality {ent._.nationality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian tourist (wife of P6)\n",
      "\tNationality Italian\n",
      "Family members of P4\n",
      "Family members of P4\n",
      "Family members of P4\n",
      "Family members of P4\n",
      "Family members of P4\n",
      "Family members of P4\n",
      "Indian who accompanied the Italian tourists\n",
      "\tNationality Indian\n",
      "\tNationality Italian\n",
      "Travelled from Italy\n",
      "\tTravel from Italy\n",
      "Travelled to Iran\n",
      "\tTravel to Iran\n",
      "Travelled to Thailand and Malaysia\n",
      "\tTravel to Thailand\n",
      "\tTravel to Malaysia\n",
      "Travelled from Iran\n",
      "\tTravel from Iran\n",
      "Travelled from Iran\n",
      "\tTravel from Iran\n",
      "Travelled from Oman\n",
      "\tTravel from Oman\n",
      "Travel from Italy on 29/02/2020 through Doha\n",
      "\tTravel from Italy\n",
      "\tTravel from Doha\n",
      "Travel from Italy on 29/02/2020 through Doha\n",
      "\tTravel from Italy\n",
      "\tTravel from Doha\n",
      "Travel from Italy on 29/02/2020 through Doha\n",
      "\tTravel from Italy\n",
      "\tTravel from Doha\n",
      "Relative and neighbour of P35-P37\n",
      "Relative  and neighbour of P35-P37\n",
      "No travel history\n"
     ]
    }
   ],
   "source": [
    "find_travelled(list(df[\"Notes\"][20:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_travelled(list(df[\"Notes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
